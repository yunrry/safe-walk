# region_risk_analyzer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
import json

class RegionRiskAnalyzer:
    """ì§€ì—­ë³„ ë§ì¶¤í˜• êµí†µì‚¬ê³  ìœ„í—˜ë„ ê¸°ì¤€ ìƒì„±ê¸°"""
    
    def __init__(self, region_data: List[Dict], region_name: str = "ì§€ì—­"):
        """
        ì´ˆê¸°í™”
        
        Args:
            region_data: ì§€ì—­ë³„ êµí†µì‚¬ê³  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            region_name: ì§€ì—­ëª… (ë¶„ì„ ê²°ê³¼ ì¶œë ¥ìš©)
        """
        self.region_data = region_data
        self.region_name = region_name
        self.df = pd.DataFrame(region_data)
        self.accidents = sorted([d['totalAccident'] for d in region_data])
        self.stats = self._calculate_comprehensive_statistics()
        self.criteria = self._generate_adaptive_criteria()
    
    def _calculate_comprehensive_statistics(self) -> Dict:
        """í¬ê´„ì ì¸ í†µê³„ ë¶„ì„"""
        accidents_array = np.array(self.accidents)
        
        # ê¸°ë³¸ í†µê³„
        basic_stats = {
            'count': len(self.accidents),
            'min': int(np.min(accidents_array)),
            'max': int(np.max(accidents_array)),
            'mean': float(np.mean(accidents_array)),
            'median': float(np.median(accidents_array)),
            'std': float(np.std(accidents_array)),
            'variance': float(np.var(accidents_array))
        }
        
        # ë¶„ìœ„ìˆ˜ í†µê³„
        percentile_stats = {
            'q1': float(np.percentile(accidents_array, 25)),
            'q3': float(np.percentile(accidents_array, 75)),
            'p10': float(np.percentile(accidents_array, 10)),
            'p90': float(np.percentile(accidents_array, 90)),
            'p33': float(np.percentile(accidents_array, 33.33)),
            'p67': float(np.percentile(accidents_array, 66.67)),
            'iqr': float(np.percentile(accidents_array, 75) - np.percentile(accidents_array, 25))
        }
        
        # ë¶„í¬ íŠ¹ì„± ë¶„ì„
        distribution_stats = {
            'skewness': self._calculate_skewness(accidents_array),
            'coefficient_of_variation': basic_stats['std'] / basic_stats['mean'] if basic_stats['mean'] > 0 else 0,
            'outlier_threshold_iqr': percentile_stats['q3'] + 1.5 * percentile_stats['iqr'],
            'outlier_threshold_std': basic_stats['mean'] + 2 * basic_stats['std']
        }
        
        # ëª¨ë“  í†µê³„ í•©ì¹˜ê¸°
        return {**basic_stats, **percentile_stats, **distribution_stats}
    
    def _calculate_skewness(self, data: np.ndarray) -> float:
        """ì™œë„(ë¹„ëŒ€ì¹­ë„) ê³„ì‚°"""
        if len(data) < 3:
            return 0.0
        
        mean = np.mean(data)
        std = np.std(data)
        
        if std == 0:
            return 0.0
        
        skewness = np.mean(((data - mean) / std) ** 3)
        return float(skewness)
    
    def _generate_adaptive_criteria(self) -> Dict:
        """
        ì§€ì—­ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì ì‘í˜• ê¸°ì¤€ ìƒì„±
        
        Returns:
            ìœ„í—˜ë„ë³„ ë²”ìœ„ ë”•ì…”ë„ˆë¦¬
        """
        stats = self.stats
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì„ê³„ê°’ ê³„ì‚°
        median_threshold = max(int(stats['median']), 1)
        mean_based_low = max(int(stats['mean'] * 0.4), 1)
        mean_based_high = max(int(stats['mean'] * 1.2), int(stats['q3']))
        
        # 2ë‹¨ê³„: ë¶„í¬ íŠ¹ì„±ì— ë”°ë¥¸ ì¡°ì •
        if stats['skewness'] > 1.0:  # ì‹¬í•˜ê²Œ ì¹˜ìš°ì¹œ ë¶„í¬
            # ê·¹ê°’ì˜ ì˜í–¥ì„ ì¤„ì´ê³  ì¤‘ì•™ê°’ ì¤‘ì‹¬ìœ¼ë¡œ ì¡°ì •
            low_threshold = max(median_threshold, int(stats['p33']))
            high_threshold = min(mean_based_high, int(stats['p90']))
        elif stats['coefficient_of_variation'] > 1.5:  # ë³€ë™ì„±ì´ í° ê²½ìš°
            # Q1, Q3 ê¸°ì¤€ìœ¼ë¡œ ì•ˆì •ì ì¸ êµ¬ê°„ ì„¤ì •
            low_threshold = max(int(stats['q1']), 1)
            high_threshold = int(stats['q3'])
        else:  # ì¼ë°˜ì ì¸ ë¶„í¬
            # í‰ê· ê³¼ ì¤‘ì•™ê°’ì„ í˜¼í•©í•œ ê¸°ì¤€
            low_threshold = max(median_threshold, mean_based_low)
            high_threshold = mean_based_high
        
        # 3ë‹¨ê³„: ì§€ì—­ ê·œëª¨ì— ë”°ë¥¸ ë¯¸ì„¸ ì¡°ì •
        region_size = stats['count']
        if region_size < 20:  # ì†Œê·œëª¨ ì§€ì—­
            # ë” ì„¸ë°€í•œ êµ¬ë¶„ì„ ìœ„í•´ ê¸°ì¤€ì„ ë‚®ì¶¤
            low_threshold = max(1, int(low_threshold * 0.8))
            high_threshold = max(low_threshold + 2, int(high_threshold * 0.9))
        elif region_size > 100:  # ëŒ€ê·œëª¨ ì§€ì—­
            # ë” ë„“ì€ ë²”ìœ„ë¡œ ê¸°ì¤€ì„ ë†’ì„
            low_threshold = int(low_threshold * 1.1)
            high_threshold = int(high_threshold * 1.1)
        
        # 4ë‹¨ê³„: ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ë³´
        if high_threshold <= low_threshold:
            high_threshold = low_threshold + max(2, int(stats['std'] * 0.5))
        
        criteria = {
            'low': {'min': stats['min'], 'max': low_threshold},
            'medium': {'min': low_threshold + 1, 'max': high_threshold},
            'high': {'min': high_threshold + 1, 'max': stats['max']}
        }
        
        return criteria
    
    def classify_regions(self) -> Dict:
        """ì§€ì—­ë³„ ìœ„í—˜ë„ ë¶„ë¥˜"""
        classified = {'low': [], 'medium': [], 'high': []}
        
        for region in self.region_data:
            accidents = region['totalAccident']
            region_copy = region.copy()
            
            if self.criteria['low']['min'] <= accidents <= self.criteria['low']['max']:
                region_copy['riskLevel'] = 'ì €ìœ„í—˜'
                classified['low'].append(region_copy)
            elif self.criteria['medium']['min'] <= accidents <= self.criteria['medium']['max']:
                region_copy['riskLevel'] = 'ì¤‘ìœ„í—˜'
                classified['medium'].append(region_copy)
            else:
                region_copy['riskLevel'] = 'ê³ ìœ„í—˜'
                classified['high'].append(region_copy)
        
        return classified
    
    def print_analysis_report(self):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì¶œë ¥"""
        classified = self.classify_regions()
        
        print(f"\n{'='*15} {self.region_name} êµí†µì‚¬ê³  ìœ„í—˜ë„ ë¶„ì„ ë³´ê³ ì„œ {'='*15}")
        
        # 1. ì§€ì—­ ê°œìš”
        print(f"\nğŸ“ ì§€ì—­ ê°œìš”:")
        print(f"   â€¢ ì´ ë²•ì •ë™ ìˆ˜: {self.stats['count']}ê°œ")
        print(f"   â€¢ ì‚¬ê³  ë²”ìœ„: {self.stats['min']}ê±´ ~ {self.stats['max']}ê±´")
        print(f"   â€¢ ì´ ì‚¬ê³  ê±´ìˆ˜: {sum(self.accidents)}ê±´")
        
        # 2. í†µê³„ ë¶„ì„
        print(f"\nğŸ“Š í†µê³„ ë¶„ì„:")
        print(f"   â€¢ í‰ê· : {self.stats['mean']:.1f}ê±´")
        print(f"   â€¢ ì¤‘ì•™ê°’: {self.stats['median']:.1f}ê±´")
        print(f"   â€¢ í‘œì¤€í¸ì°¨: {self.stats['std']:.1f}ê±´")
        print(f"   â€¢ ë³€ë™ê³„ìˆ˜: {self.stats['coefficient_of_variation']:.2f}")
        print(f"   â€¢ ì™œë„: {self.stats['skewness']:.2f}")
        
        # 3. ë¶„ìœ„ìˆ˜ ì •ë³´
        print(f"\nğŸ“ˆ ë¶„ìœ„ìˆ˜ ë¶„ì„:")
        print(f"   â€¢ 1ì‚¬ë¶„ìœ„(Q1): {self.stats['q1']:.1f}ê±´")
        print(f"   â€¢ 3ì‚¬ë¶„ìœ„(Q3): {self.stats['q3']:.1f}ê±´")
        print(f"   â€¢ 90ë°±ë¶„ìœ„: {self.stats['p90']:.1f}ê±´")
        
        # 4. ì ì‘í˜• ê¸°ì¤€
        print(f"\nğŸ¯ {self.region_name} ë§ì¶¤í˜• ìœ„í—˜ë„ ê¸°ì¤€:")
        print(f"   â€¢ ì €ìœ„í—˜: {self.criteria['low']['min']} ~ {self.criteria['low']['max']}ê±´")
        print(f"   â€¢ ì¤‘ìœ„í—˜: {self.criteria['medium']['min']} ~ {self.criteria['medium']['max']}ê±´")
        print(f"   â€¢ ê³ ìœ„í—˜: {self.criteria['high']['min']}ê±´ ì´ìƒ")
        
        # 5. ë¶„ë¥˜ ê²°ê³¼
        print(f"\nğŸ˜ï¸  ìœ„í—˜ë„ë³„ ë¶„ë¥˜ ê²°ê³¼:")
        for level in ['low', 'medium', 'high']:
            level_names = {'low': 'ì €ìœ„í—˜', 'medium': 'ì¤‘ìœ„í—˜', 'high': 'ê³ ìœ„í—˜'}
            count = len(classified[level])
            percentage = (count / self.stats['count']) * 100
            print(f"   â€¢ {level_names[level]}: {count}ê°œ ({percentage:.1f}%)")
        
        # 6. ê° ìœ„í—˜ë„ë³„ ì§€ì—­ ëª©ë¡
        print(f"\nğŸ“‹ ìœ„í—˜ë„ë³„ ì§€ì—­ ëª©ë¡:")
        for level in ['high', 'medium', 'low']:  # ê³ ìœ„í—˜ë¶€í„° ì¶œë ¥
            level_names = {'low': 'ì €ìœ„í—˜', 'medium': 'ì¤‘ìœ„í—˜', 'high': 'ê³ ìœ„í—˜'}
            regions = sorted(classified[level], key=lambda x: x['totalAccident'], reverse=True)
            
            if regions:
                print(f"\n   {level_names[level]} ì§€ì—­:")
                for region in regions:
                    print(f"     - {region['name']}: {region['totalAccident']}ê±´")
    
    def get_criteria_summary(self) -> Dict:
        """ê¸°ì¤€ê°’ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            'region_name': self.region_name,
            'total_regions': self.stats['count'],
            'criteria': self.criteria,
            'statistics': {
                'mean': self.stats['mean'],
                'median': self.stats['median'],
                'std': self.stats['std'],
                'min': self.stats['min'],
                'max': self.stats['max']
            },
            'distribution_analysis': {
                'skewness': self.stats['skewness'],
                'coefficient_of_variation': self.stats['coefficient_of_variation'],
                'distribution_type': self._get_distribution_type()
            }
        }
    
    def _get_distribution_type(self) -> str:
        """ë¶„í¬ ìœ í˜• íŒë‹¨"""
        skew = self.stats['skewness']
        cv = self.stats['coefficient_of_variation']
        
        if abs(skew) < 0.5 and cv < 0.5:
            return "ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ ì•ˆì •ì  ë¶„í¬"
        elif skew > 1.0:
            return "ì‹¬í•˜ê²Œ ì¹˜ìš°ì¹œ ë¶„í¬ (ê·¹ê°’ ì¡´ì¬)"
        elif cv > 1.5:
            return "ë³€ë™ì„±ì´ í° ë¶„í¬"
        elif skew > 0.5:
            return "ìš°ì¸¡ìœ¼ë¡œ ì¹˜ìš°ì¹œ ë¶„í¬"
        else:
            return "ì¼ë°˜ì ì¸ ë¶„í¬"
    
    def save_analysis_result(self, filename: str = None):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            filename = f"region_risk_analysis_{self.region_name.replace(' ', '_')}.json"
        
        result = {
            'region_name': self.region_name,
            'analysis_date': pd.Timestamp.now().isoformat(),
            'criteria': self.criteria,
            'statistics': self.stats,
            'classified_regions': self.classify_regions(),
            'summary': self.get_criteria_summary()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def analyze_region_risk(region_data: List[Dict], region_name: str = "ë¶„ì„ì§€ì—­") -> RegionRiskAnalyzer:
    """
    ì§€ì—­ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ìœ„í—˜ë„ ê¸°ì¤€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        region_data: ì§€ì—­ë³„ êµí†µì‚¬ê³  ë°ì´í„°
        region_name: ì§€ì—­ëª…
    
    Returns:
        RegionRiskAnalyzer ê°ì²´
    """
    analyzer = RegionRiskAnalyzer(region_data, region_name)
    analyzer.print_analysis_report()
    return analyzer


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê²½ì£¼ì‹œ ë°ì´í„° ì˜ˆì‹œ
    gyeongju_data = [
        {"name": "ë™ë¶€ë™", "totalAccident": 4, "EMD_CD": "47130101", "latitude": 35.845893, "longitude": 129.212176},
        {"name": "ì™¸ë™ì", "totalAccident": 2, "EMD_CD": "47130259", "latitude": 35.738707, "longitude": 129.284858},
        {"name": "ì‹œë˜ë™", "totalAccident": 5, "EMD_CD": "47130130", "latitude": 35.77228, "longitude": 129.303108},
        {"name": "í™©ì˜¤ë™", "totalAccident": 38, "EMD_CD": "47130105", "latitude": 35.842537, "longitude": 129.22005},
        {"name": "ì„±ê±´ë™", "totalAccident": 58, "EMD_CD": "47130108", "latitude": 35.850957, "longitude": 129.207231},
        {"name": "ê°í¬ì", "totalAccident": 3, "EMD_CD": "47130250", "latitude": 35.80601, "longitude": 129.501105},
        {"name": "ì„±ë™ë™", "totalAccident": 53, "EMD_CD": "47130104", "latitude": 35.847318, "longitude": 129.217713},
        {"name": "ì•ˆê°•ì", "totalAccident": 8, "EMD_CD": "47130253", "latitude": 35.91722, "longitude": 129.229519},
        {"name": "í˜„ê³¡ë©´", "totalAccident": 3, "EMD_CD": "47130360", "latitude": 35.880682, "longitude": 129.202557},
        {"name": "ë…¸ë™ë™", "totalAccident": 4, "EMD_CD": "47130106", "latitude": 35.842643, "longitude": 129.212062},
        {"name": "í™©ì„±ë™", "totalAccident": 7, "EMD_CD": "47130124", "latitude": 35.867202, "longitude": 129.216789},
        {"name": "ì‚¬ì •ë™", "totalAccident": 13, "EMD_CD": "47130109", "latitude": 35.834371, "longitude": 129.207049},
        {"name": "ë…¸ì„œë™", "totalAccident": 12, "EMD_CD": "47130107", "latitude": 35.841499, "longitude": 129.20614}
    ]
    
    # ê²½ì£¼ì‹œ ë¶„ì„
    print("ğŸ›ï¸ ê²½ì£¼ì‹œ êµí†µì‚¬ê³  ìœ„í—˜ë„ ë¶„ì„")
    gyeongju_analyzer = analyze_region_risk(gyeongju_data, "ê²½ì£¼ì‹œ")
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    gyeongju_analyzer.save_analysis_result()
    
    # ê¸°ì¤€ê°’ë§Œ ê°„ë‹¨íˆ ì¶œë ¥
    print(f"\nğŸ¯ ê²½ì£¼ì‹œ ìµœì¢… ê¸°ì¤€ê°’:")
    criteria = gyeongju_analyzer.criteria
    print(f"ì €ìœ„í—˜: {criteria['low']['min']} ~ {criteria['low']['max']}ê±´")
    print(f"ì¤‘ìœ„í—˜: {criteria['medium']['min']} ~ {criteria['medium']['max']}ê±´")
    print(f"ê³ ìœ„í—˜: {criteria['high']['min']}ê±´ ì´ìƒ")